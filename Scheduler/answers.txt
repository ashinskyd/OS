David A., Kiley M, Jon G.Scheduler WriteupRaw Data:FIFO 1:# of Context Switches: 98Total execution time: 67.6 sTotal time spent in READY state: 390.2 sFIFO 2:# of Context Switches: 108Total execution time: 35.9 sTotal time spent in READY state: 80.5 sFIFO 4:# of Context Switches: 182Total execution time: 33.4 sTotal time spent in READY state: 0.3 s RR (1 CPU) 800ms:# of Context Switches: 135Total execution time: 67.7 sTotal time spent in READY state: 232.5 sRR 600ms:# of Context Switches: 160Total execution time: 67.6 sTotal time spent in READY state: 182.2 sRR 400ms:# of Context Switches: 203Total execution time: 67.6 sTotal time spent in READY state: 134.6 sRR 200ms:# of Context Switches: 362Total execution time: 67.5 sTotal time spent in READY state: 71.2 s Static Priority:# of Context Switches: 168Total execution time: 68.3 sTotal time spent in READY state: 28.1 sQuestion 1:       With 1 cpu the execution time was 67.6s. With 2 cpus, the time was 35.9s and with 4 cpus the time was 33.4s.  No there is not a linear relationship between cpus and execution time. In general though, the execution time decreased significantly with a jump from 1 to 2 processors, but the decrease in execution time was much less significant going from 2 to 4 processors. This is the case because as the number of processors increase, the lower bound on execution time begins to approach the i/o-bound processes. In other words, the bottle neck is no longer on processes waiting for cpu time, but instead becomes processes waiting for io before they can run.Question 2:      Firstly, the data for all the round robin time slices shows that all had roughly the same total execution time. This makes sense for round robin since the time slices were significantly less than the execution time for a single process, so most processes were preempted off the cpu at some point, giving each roughly the same amount of running time in each scenario (no matter the time slice). In addition, as the time slice decreased, the number of context switches increased which also makes sense.       However, as time slices got smaller, the amount of time spent waiting (in the ready state) decreased. This makes sense for two main reasons. First, for very short cpu bound processes, they will be able to get on the cpu faster (waiting less), allowing them to execute sooner, and stop blocking other processes from waiting. Secondly, for i/o bound processes, since the ready queue is cycled more often, they are able to receiver their respective io more often, and complete their tasks, thus reducing further blocking on the queue.	In a real OS, it is not usually the best choice to have the shortest possible time slice because context switches can be rather costly. As you decrease the time slice, you also increase the number of context switches, which means that you become less efficient. Question 3:	In our case, the static priority had the shortest waiting time of all the scheduling algorithms. In this specific case, for these processes and these priorities, it just so happened that static priority is the best approximation for SJF. However, in general, if you use static priority scheduling where the priorities correspond to estimated job lengths, then you’re essentially mimicking SJF by estimation. 	However, in general, it is the case that round robin with small quanta will result in the best approximation for SJF. This is the case because the shorter quanta allow the shorter jobs to be finished quickly since you won’t be blocked by the length of the longer job.  